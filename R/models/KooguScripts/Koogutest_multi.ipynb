{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from koogu.data import preprocess, feeder\n",
    "from koogu.model import architectures\n",
    "from koogu import train, assessments, recognize\n",
    "\n",
    "from matplotlib import pyplot as plt           # used for plotting graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List class-specific subdirectories to process\n",
    "class_dirs = ['CrestedGibbons','GreyGibbons', 'noise']\n",
    "\n",
    "# Path to the directory where pre-processed data will be written.\n",
    "# Directory will be created if it doesn't exist.\n",
    "prepared_audio_dir = '/Volumes/DJC Files/MultiSpeciesTransferLearning/TrainingDataWavs/CombinedClipsBirdNET/'\n",
    "\n",
    "data_settings = {\n",
    "    # Settings for handling raw audio\n",
    "    'audio_settings': {\n",
    "        'clip_length': 12.0,\n",
    "        'clip_advance': 0.4,\n",
    "        'desired_fs': 32000\n",
    "    },\n",
    "\n",
    "    # Settings for converting audio to a time-frequency representation\n",
    "    'spec_settings': {\n",
    "        'win_len': 0.128,\n",
    "        'win_overlap_prc': 0.75,\n",
    "        'bandwidth_clip': [500, 3000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert audio files into prepared data\n",
    "clip_counts = preprocess.from_top_level_dirs(\n",
    "    data_settings['audio_settings'],\n",
    "    class_dirs=class_dirs,\n",
    "    audio_root=prepared_audio_dir,\n",
    "    output_root='/Users/denaclink/Desktop/VSCodeRepos/BEANS/data_multi/',\n",
    "    negative_class_label='noise')\n",
    "\n",
    "print(clip_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_audio_dir = '/Users/denaclink/Desktop/VSCodeRepos/BEANS/data_multi/'\n",
    "\n",
    "data_feeder = feeder.SpectralDataFeeder(\n",
    "    prepared_audio_dir,                        # where the prepared clips are at\n",
    "    data_settings['audio_settings']['desired_fs'],\n",
    "    data_settings['spec_settings'],\n",
    "    validation_split=0.15,                     # set aside 15% for validation\n",
    "    max_clips_per_class=20000                  # use up to 20k inputs per class\n",
    ")\n",
    "\n",
    "print(data_feeder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = architectures.DenseNet(\n",
    "    [4, 4, 4],                                 # 3 dense-blocks, 4 layers each\n",
    "    preproc=[ ('Conv2D', {'filters': 16}) ],   # Add a 16-filter pre-conv layer\n",
    "    dense_layers=[32]                          # End with a 32-node dense layer\n",
    ")\n",
    "\n",
    "# Settings that control the training process\n",
    "training_settings = {\n",
    "    'batch_size': 64,\n",
    "    'epochs': 80,                              # run for 50 epochs\n",
    "\n",
    "    # Start with a learning rate of 0.01, and drop it to a tenth of its value,\n",
    "    # successively, at epochs 20 & 40.\n",
    "    'learning_rate': 0.01,\n",
    "    'lr_change_at_epochs': [20, 40],\n",
    "    'lr_update_factors': [1.0, 1e-1, 1e-2],    # up to 20, beyond 20, beyond 40\n",
    "\n",
    "    'dropout_rate': 0.05                       # Helps model generalize better\n",
    "}\n",
    "\n",
    "# Path to the directory where model files will be written\n",
    "model_dir = '/Users/denaclink/Desktop/VSCodeRepos/BEANS/model_multi/'\n",
    "\n",
    "# Perform training\n",
    "history = train(\n",
    "    data_feeder,\n",
    "    model_dir,\n",
    "    data_settings,\n",
    "    model,\n",
    "    training_settings\n",
    ")\n",
    "\n",
    "# Plot training & validation history\n",
    "fig, ax = plt.subplots(2, sharex=True, figsize=(12, 9))\n",
    "ax[0].plot(\n",
    "    history['train_epochs'], history['binary_accuracy'], 'r',\n",
    "    history['eval_epochs'], history['val_binary_accuracy'], 'g')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[1].plot(\n",
    "    history['train_epochs'], history['loss'], 'r',\n",
    "    history['eval_epochs'], history['val_loss'], 'g')\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from koogu import recognize\n",
    "\n",
    "# Path to a single audio file or to a directory (can contain subdirectories)\n",
    "test_audio_root = \"/Volumes/DJC Files/MultiSpeciesTransferLearning/WideArrayEvaluation/Jahoo/ImagesIgnoreWindowsWavs/\"\n",
    "\n",
    "# Output directory\n",
    "raw_detections_root = '/Users/denaclink/Desktop/VSCodeRepos/BEANS/detections_multi/'\n",
    "chosen_threshold = 0.1\n",
    "\n",
    "recognize(\n",
    "  model_dir='/Users/denaclink/Desktop/VSCodeRepos/BEANS/model_multi/',\n",
    "  audio_root=test_audio_root,\n",
    "  output_dir=raw_detections_root,\n",
    "  threshold=chosen_threshold,\n",
    "  batch_size=64,    # Increasing this may improve speed on computers having higher resources\n",
    "  recursive=True,   # Process subdirectories also\n",
    "  show_progress=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koogu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
